# Technologies-of-Deep-Learning

## Лабораторная 1: DenseNet, Adam vs RAdam

Обучение производилось в течение 50 эпох.
Самые лучшие результаты были на 47-ой эпохе, дальше обучение было остановлено, чтобы не допустить сильного переобучения.

На графиках в Jupyter Notebook видно, что сходимость при оптимайзере RAdam быстрее, чем при Adam. На тестовых данных, несмотря на то, что результаты лоссов с Adam более монотонные и не имеют сильных скачков, они всё же хуже, чем у RAdam.

Точность на train у Adam и RAdam становится примерно одинаковой с 35-ой эпохи (0.9+), но на тесте показатели RAdam в этих моментах примерно постоянно на 0.1 выше (0.6 - 0.67), чем у Adam (0.5 - 0.58)
